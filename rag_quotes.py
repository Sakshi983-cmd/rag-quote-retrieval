# -*- coding: utf-8 -*-
"""rag quotes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11R4JtpgoRGZZev4G79VOV4-KJjizz9H8
"""

# RAG-Based Quote Retrieval System (Streamlit App)

# Step 1: Install Required Libraries (only in Colab or local environment)
# !pip install datasets sentence-transformers faiss-cpu streamlit transformers

import streamlit as st
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Step 2: Load Dataset
def load_quotes():
    dataset = load_dataset("Abirate/english_quotes", split="train")
    df = pd.DataFrame(dataset)
    df = df.dropna(subset=['quote', 'author', 'tags'])
    return df.reset_index(drop=True)

# Step 3: Create Embeddings and FAISS Index
def build_faiss_index(quotes, model):
    embeddings = model.encode(quotes, show_progress_bar=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, embeddings

# Step 4: Search Quotes using FAISS
def search_quotes(query, model, index, quotes_df, top_k=5):
    query_vec = model.encode([query])
    distances, indices = index.search(np.array(query_vec), top_k)
    results = []
    for i in indices[0]:
        row = quotes_df.iloc[i]
        results.append({
            'quote': row['quote'],
            'author': row['author'],
            'tags': row['tags']
        })
    return results

# Step 5: Generate Answer using Open-Source LLM
def generate_answer(contexts, query):
    prompt = """
Context:
"""
    for i, ctx in enumerate(contexts):
        prompt += f"{i+1}. \"{ctx['quote']}\" - {ctx['author']} [{ctx['tags']}]\n"
    prompt += f"\nUser Query: {query}\nAnswer: "

    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=100)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer.split("Answer:")[-1].strip()

# Step 6: Streamlit UI
st.title("ðŸ§  Semantic Quote Finder (RAG)")
st.markdown("Search and retrieve meaningful quotes using Retrieval-Augmented Generation")

query = st.text_input("Enter your quote query (e.g., quotes about courage by women):")

@st.cache_resource
def init():
    df = load_quotes()
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')
    index, _ = build_faiss_index(df['quote'].tolist(), embed_model)
    return df, embed_model, index

df, embed_model, index = init()

if query:
    with st.spinner("Retrieving quotes..."):
        top_quotes = search_quotes(query, embed_model, index, df)
        st.subheader("ðŸ“œ Retrieved Quotes:")
        st.json(top_quotes)

        # Optional: Use LLM to generate final answer
        st.subheader("ðŸ’¡ Generated Answer:")
        try:
            answer = generate_answer(top_quotes, query)
            st.write(answer)
        except Exception as e:
            st.warning("Could not load Mistral model. Showing raw quotes only.")